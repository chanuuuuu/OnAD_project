{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from komoran3py import KomoranPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko = KomoranPy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./df_1.pickle', 'rb') as f : \n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>emo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>배경음 love theme의 감동적인 선율과 영사기에서 나오는 달콤한 키스 장면들을...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>불멸의 명작. 영화인을 꿈꾸는 사람이라면 반드시 봐야할 영화</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>탄탄한 스토리와 거장 엔니오 모리꼬네가 만들어낸 최고의 영화. 몇번을 봐도 마지막장...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>20년이 훨씬 지났지만 아직도 생각하면 가슴이 벅차네요...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>오랜만에 다시 보아도 너무 예쁘고 사랑스런 눈물나게 하는 영화</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>지금 극장에 걸려 있는 건 축약판(123분). 축약판만 보신 분들은 꼭 감독판(17...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>아 근데 재개봉 버전 뭐이렇게 삭제된 장면이 많냐;; 후반 엘레나 시퀀스는 통째로 ...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>영화관에서 절대 보지말아라 이야기를 끌어가는 주요 이야기인 엘레나와 토토의 재회부분...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>삶의 모든 것이 들어있었다 ㅠㅠ</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>10대에 보았다 그때는 왜 눈물이 그치지 않는지 알수가 없었다. 30중반 이제서 다...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>신촌cgv에서 예전시네마천국  기대했는데   너무영화가  뚝뚝 끊어지냉 ㄴ누가  편...</td>\n",
       "      <td>NEUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9</td>\n",
       "      <td>살바토레와 알프레도의 케미는 가히 역대급이다... 명작의 가치는 역시 시대를 초월한다</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>아주  먼  훗날, 이제  더는  흘릴  눈물이 남아 있을 것 같지  않을 그 날에...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>무슨 네오리얼리즘 영화도 아니고 구지 해석하고 분석하려 하지 말아라.그냥 그대로 보...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>ost만 들어도 눈물나는 영화</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>왜 죽기 전에 꼭 봐야 하는 영화인지 알겠습니다. 오늘 12시 15분에 관람했는데 ...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>마지막 장면은 절대 잊을 수 없을 것이다!</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>살아가는 동안 누구나 하나쯤 시네마천국을 가질것이다. 이영화는 나의 시네마천국을 생...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>제 인생 최고의 영화였습니다.아직도 이보다 더한 감동과그리움을 느끼는 영화는 만나지...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>너무슬프다.. 말그대로 명작이다</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>방금 감독판 봤는데 이런 걸작을 지금까지 못본게 후회가 될만큼 큰 감명을 받았네요....</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9</td>\n",
       "      <td>최고의 영화를 지금 다시 볼 수 있어서 좋았습니다.</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>1990년 고1때 영화관에서 돈주고 2번 본영화~친척언니랑 여름방학때보고 넘감동적인...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>시대는 변하고 기억은 변하지 않기때문에아름답게 추억할수있는 과거</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>마지막 장면은 울지 않고는 볼 수 없었다는...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>영화는 무엇일까? 인생의 또다른 말이 아닐까?</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>단언컨대, 시네마 천국은 최고의 영화입니다</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10</td>\n",
       "      <td>이 영화에 10점 못 주는 사람은 가슴 어딘가에 이상이 있지 않을까?</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10</td>\n",
       "      <td>어떠한 긴 말보다 엄청난 감동이 밀려오는 영화입니다.</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9</td>\n",
       "      <td>역시 엔리오모리꼬네 영화음악은 말할것없고. 영화에 대한순수한마음이느껴져 좋았다 일상...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>1</td>\n",
       "      <td>홍성빈같은 영화 한마디로 별로</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>1</td>\n",
       "      <td>재밌지만 너무높은듯..</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3437</th>\n",
       "      <td>1</td>\n",
       "      <td>솔직히 길고 지루하다.</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3438</th>\n",
       "      <td>1</td>\n",
       "      <td>평점이해안감;;;;;;;</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3439</th>\n",
       "      <td>1</td>\n",
       "      <td>너무 진부하고 너무 지루하다 난 또 대단한 영화인 줄 알고 봤더니 그냥 뻔한 멜로물</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>1</td>\n",
       "      <td>남들이 10점줄때 1점 줄 수 있는 용기도 필요하다!</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3441</th>\n",
       "      <td>1</td>\n",
       "      <td>ㅂㅅ들이냐 이탈리아사람들한테만 특별한의미가 있어 평점이높은건데 우리나라사남이 보고 ...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442</th>\n",
       "      <td>1</td>\n",
       "      <td>졸작</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3443</th>\n",
       "      <td>1</td>\n",
       "      <td>비추</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>1</td>\n",
       "      <td>이거 포르노임? 주인공 변태,불륜 스토커네 OO..내생에 최악의 영화!!!</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3445</th>\n",
       "      <td>1</td>\n",
       "      <td>알프레도... 알고 보면 나쁜 놈이다. 솔직히 토토가 엘레나와 헤어지게 된 것은 엘...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3446</th>\n",
       "      <td>1</td>\n",
       "      <td>성매매,스토킹,불륜... 이런 쓰레기 영화가 어떻게 전체관람가냐?</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>2</td>\n",
       "      <td>개노잼인데????왜그렇게 극찬하는거야대체?</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>1</td>\n",
       "      <td>평점 1위라 봣는데 개 별로ㅡㅡ</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>1</td>\n",
       "      <td>평점 높아서 기대하고 봤는데 정말 실망이 크네요</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>1</td>\n",
       "      <td>평점깎깅ㅍㅅㄴㅋ다라</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>1</td>\n",
       "      <td>몇번이나 시도했지만 매번 끝까지 완주하지 못했다</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452</th>\n",
       "      <td>1</td>\n",
       "      <td>워낙 이름난 작품이라 봤는데, 저는 왠지 별 감흥이 없네요. 나중에 다시 한번 볼께요.</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3453</th>\n",
       "      <td>1</td>\n",
       "      <td>아아아아오아아아오어어어</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3454</th>\n",
       "      <td>1</td>\n",
       "      <td>너무 좋은 영화인거 같다. 한번 더 볼 생각이 있음</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3455</th>\n",
       "      <td>1</td>\n",
       "      <td>뭐야이건완전옛날영화왜다시개봉하는지</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3456</th>\n",
       "      <td>1</td>\n",
       "      <td>제밌나요? 이런우연가능한가요?</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3457</th>\n",
       "      <td>1</td>\n",
       "      <td>음.. 뭔가 기대이하야</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3458</th>\n",
       "      <td>1</td>\n",
       "      <td>고마 내리자 마 할만큼햇다 아니가</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3459</th>\n",
       "      <td>1</td>\n",
       "      <td>아 정말 다좋은데 왜 하필 중년의 살바토레가 전혀매치안되는 인물로 나온거야....</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>1</td>\n",
       "      <td>최악의영화인것같았다.</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>1</td>\n",
       "      <td>과대 평가된 영화라고 생각됨....</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>1</td>\n",
       "      <td>왠지영화랑인물이매치가안된다.</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>1</td>\n",
       "      <td>언제적영화냐 그만하자ㅋㅋ모야이건</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>1</td>\n",
       "      <td>평점 너무 높음;;</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3465 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                             review   emo\n",
       "0         10  배경음 love theme의 감동적인 선율과 영사기에서 나오는 달콤한 키스 장면들을...   POS\n",
       "1          9                  불멸의 명작. 영화인을 꿈꾸는 사람이라면 반드시 봐야할 영화   POS\n",
       "2         10  탄탄한 스토리와 거장 엔니오 모리꼬네가 만들어낸 최고의 영화. 몇번을 봐도 마지막장...   POS\n",
       "3         10                  20년이 훨씬 지났지만 아직도 생각하면 가슴이 벅차네요...   POS\n",
       "4         10                 오랜만에 다시 보아도 너무 예쁘고 사랑스런 눈물나게 하는 영화   POS\n",
       "5         10  지금 극장에 걸려 있는 건 축약판(123분). 축약판만 보신 분들은 꼭 감독판(17...   POS\n",
       "6         10  아 근데 재개봉 버전 뭐이렇게 삭제된 장면이 많냐;; 후반 엘레나 시퀀스는 통째로 ...   POS\n",
       "7          1  영화관에서 절대 보지말아라 이야기를 끌어가는 주요 이야기인 엘레나와 토토의 재회부분...   NEG\n",
       "8         10                                  삶의 모든 것이 들어있었다 ㅠㅠ   POS\n",
       "9         10  10대에 보았다 그때는 왜 눈물이 그치지 않는지 알수가 없었다. 30중반 이제서 다...   POS\n",
       "10         6  신촌cgv에서 예전시네마천국  기대했는데   너무영화가  뚝뚝 끊어지냉 ㄴ누가  편...  NEUT\n",
       "11         9    살바토레와 알프레도의 케미는 가히 역대급이다... 명작의 가치는 역시 시대를 초월한다   POS\n",
       "12        10  아주  먼  훗날, 이제  더는  흘릴  눈물이 남아 있을 것 같지  않을 그 날에...   POS\n",
       "13        10  무슨 네오리얼리즘 영화도 아니고 구지 해석하고 분석하려 하지 말아라.그냥 그대로 보...   POS\n",
       "14        10                                   ost만 들어도 눈물나는 영화   POS\n",
       "15        10  왜 죽기 전에 꼭 봐야 하는 영화인지 알겠습니다. 오늘 12시 15분에 관람했는데 ...   POS\n",
       "16        10                            마지막 장면은 절대 잊을 수 없을 것이다!   POS\n",
       "17        10  살아가는 동안 누구나 하나쯤 시네마천국을 가질것이다. 이영화는 나의 시네마천국을 생...   POS\n",
       "18        10  제 인생 최고의 영화였습니다.아직도 이보다 더한 감동과그리움을 느끼는 영화는 만나지...   POS\n",
       "19        10                                  너무슬프다.. 말그대로 명작이다   POS\n",
       "20        10  방금 감독판 봤는데 이런 걸작을 지금까지 못본게 후회가 될만큼 큰 감명을 받았네요....   POS\n",
       "21         9                       최고의 영화를 지금 다시 볼 수 있어서 좋았습니다.   POS\n",
       "22        10  1990년 고1때 영화관에서 돈주고 2번 본영화~친척언니랑 여름방학때보고 넘감동적인...   POS\n",
       "23         8                시대는 변하고 기억은 변하지 않기때문에아름답게 추억할수있는 과거   POS\n",
       "24        10                         마지막 장면은 울지 않고는 볼 수 없었다는...   POS\n",
       "25         8                          영화는 무엇일까? 인생의 또다른 말이 아닐까?   POS\n",
       "26        10                            단언컨대, 시네마 천국은 최고의 영화입니다   POS\n",
       "27        10             이 영화에 10점 못 주는 사람은 가슴 어딘가에 이상이 있지 않을까?   POS\n",
       "28        10                      어떠한 긴 말보다 엄청난 감동이 밀려오는 영화입니다.   POS\n",
       "29         9  역시 엔리오모리꼬네 영화음악은 말할것없고. 영화에 대한순수한마음이느껴져 좋았다 일상...   POS\n",
       "...      ...                                                ...   ...\n",
       "3435       1                                   홍성빈같은 영화 한마디로 별로   NEG\n",
       "3436       1                                       재밌지만 너무높은듯..   NEG\n",
       "3437       1                                       솔직히 길고 지루하다.   NEG\n",
       "3438       1                                      평점이해안감;;;;;;;   NEG\n",
       "3439       1     너무 진부하고 너무 지루하다 난 또 대단한 영화인 줄 알고 봤더니 그냥 뻔한 멜로물   NEG\n",
       "3440       1                      남들이 10점줄때 1점 줄 수 있는 용기도 필요하다!   NEG\n",
       "3441       1  ㅂㅅ들이냐 이탈리아사람들한테만 특별한의미가 있어 평점이높은건데 우리나라사남이 보고 ...   NEG\n",
       "3442       1                                                 졸작   NEG\n",
       "3443       1                                                 비추   NEG\n",
       "3444       1          이거 포르노임? 주인공 변태,불륜 스토커네 OO..내생에 최악의 영화!!!   NEG\n",
       "3445       1  알프레도... 알고 보면 나쁜 놈이다. 솔직히 토토가 엘레나와 헤어지게 된 것은 엘...   NEG\n",
       "3446       1               성매매,스토킹,불륜... 이런 쓰레기 영화가 어떻게 전체관람가냐?   NEG\n",
       "3447       2                            개노잼인데????왜그렇게 극찬하는거야대체?   NEG\n",
       "3448       1                                  평점 1위라 봣는데 개 별로ㅡㅡ   NEG\n",
       "3449       1                         평점 높아서 기대하고 봤는데 정말 실망이 크네요   NEG\n",
       "3450       1                                         평점깎깅ㅍㅅㄴㅋ다라   NEG\n",
       "3451       1                         몇번이나 시도했지만 매번 끝까지 완주하지 못했다   NEG\n",
       "3452       1   워낙 이름난 작품이라 봤는데, 저는 왠지 별 감흥이 없네요. 나중에 다시 한번 볼께요.   NEG\n",
       "3453       1                                       아아아아오아아아오어어어   NEG\n",
       "3454       1                       너무 좋은 영화인거 같다. 한번 더 볼 생각이 있음   NEG\n",
       "3455       1                                 뭐야이건완전옛날영화왜다시개봉하는지   NEG\n",
       "3456       1                                   제밌나요? 이런우연가능한가요?   NEG\n",
       "3457       1                                       음.. 뭔가 기대이하야   NEG\n",
       "3458       1                                 고마 내리자 마 할만큼햇다 아니가   NEG\n",
       "3459       1      아 정말 다좋은데 왜 하필 중년의 살바토레가 전혀매치안되는 인물로 나온거야....   NEG\n",
       "3460       1                                        최악의영화인것같았다.   NEG\n",
       "3461       1                                과대 평가된 영화라고 생각됨....   NEG\n",
       "3462       1                                    왠지영화랑인물이매치가안된다.   NEG\n",
       "3463       1                                  언제적영화냐 그만하자ㅋㅋ모야이건   NEG\n",
       "3464       1                                         평점 너무 높음;;   NEG\n",
       "\n",
       "[3465 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( df['review'], df['emo']\n",
    "                                                                  ,test_size=0.3, random_state=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(x) : \n",
    "    if x == 'POS' : \n",
    "        x = 1\n",
    "        return x\n",
    "    elif x == 'NEUT' : \n",
    "        x = 0\n",
    "        return x\n",
    "    else : \n",
    "        x = -1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['y_test'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['y_test'] = df_test['y_test'].apply(lambda x : label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(df_test,'./df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_komoran(x) : \n",
    "    tmp_list = []\n",
    "    tmp = ko.pos(x)\n",
    "    for i in range(len(tmp)) : \n",
    "        tmp_list.append(tmp[i][0])\n",
    "    return tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['X_test_tokkend'] = df_test['review'].apply(lambda x :  tokenizer_komoran(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenizer_komoran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_nbc = Pipeline([('vect', tfidf), ('nbc', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.684580s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "multi_nbc.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print('Time: {:f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = multi_nbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 정확도: 0.916\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 정확도: {:.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = Pipeline([('vect', tfidf), ('sgd', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.687364s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print('Time: {:f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 정확도: 0.916\n"
     ]
    }
   ],
   "source": [
    "y_pred = multi_nbc.predict(X_test)\n",
    "print(\"테스트 정확도: {:.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['tokens'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-47bfc3b4268a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtagged_train_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'emo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtagged_test_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_test_tokkend'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2726\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['tokens'] not in index\""
     ]
    }
   ],
   "source": [
    "tagged_train_docs = [TaggedDocument(d, c) for d, c in df[['tokens', 'emo']].values]\n",
    "tagged_test_docs = [TaggedDocument(d, c) for d, c in df_test[['X_test_tokkend', 'y_test']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tagged_test_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2bd3e9e0ed27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_test_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_train_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tagged_test_docs' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(tagged_test_docs), len(tagged_train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "doc_vectorizer = Doc2Vec(\n",
    "    dm=0,            # PV-DBOW / default 1\n",
    "    dbow_words=1,    # w2v simultaneous with DBOW d2v / default 0\n",
    "    window=8,        # distance between the predicted word and context words\n",
    "    size=300,        # vector size\n",
    "    alpha=0.025,     # learning-rate\n",
    "    seed=1234,\n",
    "    min_count=20,    # ignore with freq lower\n",
    "    min_alpha=0.025, # min learning-rate\n",
    "    workers=cores,   # multi cpu\n",
    "    hs = 1,          # hierarchical softmax / default 0\n",
    "    negative = 10,   # negative sampling / default 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 11:48:39,947 : INFO : collecting all words and their counts\n",
      "2019-01-14 11:48:39,948 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-01-14 11:48:39,984 : INFO : collected 3387 word types and 8 unique tags from a corpus of 3465 examples and 50583 words\n",
      "2019-01-14 11:48:39,985 : INFO : Loading a fresh vocabulary\n",
      "2019-01-14 11:48:39,988 : INFO : effective_min_count=20 retains 305 unique words (9% of original 3387, drops 3082)\n",
      "2019-01-14 11:48:39,988 : INFO : effective_min_count=20 leaves 41825 word corpus (82% of original 50583, drops 8758)\n",
      "2019-01-14 11:48:39,990 : INFO : deleting the raw counts dictionary of 3387 items\n",
      "2019-01-14 11:48:39,991 : INFO : sample=0.001 downsamples 79 most-common words\n",
      "2019-01-14 11:48:39,992 : INFO : downsampling leaves estimated 22620 word corpus (54.1% of prior 41825)\n",
      "2019-01-14 11:48:39,992 : INFO : constructing a huffman tree from 305 words\n",
      "2019-01-14 11:48:39,998 : INFO : built huffman tree with maximum node depth 11\n",
      "2019-01-14 11:48:39,999 : INFO : estimated required memory for 305 words and 300 dimensions: 1322700 bytes\n",
      "2019-01-14 11:48:40,000 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d300,n10,hs,w8,mc20,s0.001,t4)\n"
     ]
    }
   ],
   "source": [
    "doc_vectorizer.build_vocab(tagged_train_docs)\n",
    "print(str(doc_vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3465"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectorizer.corpus_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectorizer.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n",
      "2019-01-14 11:49:22,977 : INFO : training model with 4 workers on 305 vocabulary and 300 features, using sg=1 hs=1 sample=0.001 negative=10 window=8\n",
      "2019-01-14 11:49:23,333 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:23,339 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:23,344 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:23,434 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:23,434 : INFO : EPOCH - 1 : training on 50583 raw words (33123 effective words) took 0.4s, 73726 effective words/s\n",
      "2019-01-14 11:49:23,777 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:23,783 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:23,784 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:23,881 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:23,881 : INFO : EPOCH - 2 : training on 50583 raw words (33127 effective words) took 0.4s, 75155 effective words/s\n",
      "2019-01-14 11:49:24,221 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:24,227 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:24,233 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:24,327 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:24,328 : INFO : EPOCH - 3 : training on 50583 raw words (33282 effective words) took 0.4s, 75633 effective words/s\n",
      "2019-01-14 11:49:24,666 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:24,670 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:24,675 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:24,769 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:24,770 : INFO : EPOCH - 4 : training on 50583 raw words (33130 effective words) took 0.4s, 75947 effective words/s\n",
      "2019-01-14 11:49:25,087 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:25,097 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:25,102 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:25,194 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:25,194 : INFO : EPOCH - 5 : training on 50583 raw words (33084 effective words) took 0.4s, 79034 effective words/s\n",
      "2019-01-14 11:49:25,195 : INFO : training on a 252915 raw words (165746 effective words) took 2.2s, 74780 effective words/s\n",
      "2019-01-14 11:49:25,195 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-01-14 11:49:25,196 : INFO : training model with 4 workers on 305 vocabulary and 300 features, using sg=1 hs=1 sample=0.001 negative=10 window=8\n",
      "2019-01-14 11:49:25,532 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:25,544 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:25,559 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:25,642 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:25,643 : INFO : EPOCH - 1 : training on 50583 raw words (33074 effective words) took 0.4s, 75217 effective words/s\n",
      "2019-01-14 11:49:25,982 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:25,987 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:25,989 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:26,079 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:26,080 : INFO : EPOCH - 2 : training on 50583 raw words (32992 effective words) took 0.4s, 76888 effective words/s\n",
      "2019-01-14 11:49:26,417 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:26,422 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:26,426 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:26,517 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:26,518 : INFO : EPOCH - 3 : training on 50583 raw words (32898 effective words) took 0.4s, 76071 effective words/s\n",
      "2019-01-14 11:49:26,833 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:26,837 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:26,845 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:26,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:26,941 : INFO : EPOCH - 4 : training on 50583 raw words (33210 effective words) took 0.4s, 79835 effective words/s\n",
      "2019-01-14 11:49:27,286 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:27,292 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:27,296 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:27,389 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:27,390 : INFO : EPOCH - 5 : training on 50583 raw words (33051 effective words) took 0.4s, 74833 effective words/s\n",
      "2019-01-14 11:49:27,390 : INFO : training on a 252915 raw words (165225 effective words) took 2.2s, 75348 effective words/s\n",
      "2019-01-14 11:49:27,391 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-01-14 11:49:27,392 : INFO : training model with 4 workers on 305 vocabulary and 300 features, using sg=1 hs=1 sample=0.001 negative=10 window=8\n",
      "2019-01-14 11:49:27,722 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:27,725 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:27,728 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:27,823 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:27,823 : INFO : EPOCH - 1 : training on 50583 raw words (33087 effective words) took 0.4s, 77824 effective words/s\n",
      "2019-01-14 11:49:28,157 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:28,159 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:28,166 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:28,256 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:28,256 : INFO : EPOCH - 2 : training on 50583 raw words (33117 effective words) took 0.4s, 77500 effective words/s\n",
      "2019-01-14 11:49:28,584 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:28,592 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:28,600 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:28,693 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:28,694 : INFO : EPOCH - 3 : training on 50583 raw words (33038 effective words) took 0.4s, 76672 effective words/s\n",
      "2019-01-14 11:49:29,026 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:29,030 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:29,038 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:29,128 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:29,129 : INFO : EPOCH - 4 : training on 50583 raw words (33108 effective words) took 0.4s, 77154 effective words/s\n",
      "2019-01-14 11:49:29,458 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:29,471 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 11:49:29,473 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:29,564 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:29,565 : INFO : EPOCH - 5 : training on 50583 raw words (32924 effective words) took 0.4s, 76590 effective words/s\n",
      "2019-01-14 11:49:29,566 : INFO : training on a 252915 raw words (165274 effective words) took 2.2s, 76032 effective words/s\n",
      "2019-01-14 11:49:29,567 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-01-14 11:49:29,568 : INFO : training model with 4 workers on 305 vocabulary and 300 features, using sg=1 hs=1 sample=0.001 negative=10 window=8\n",
      "2019-01-14 11:49:29,914 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:29,920 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:29,924 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:30,018 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:30,020 : INFO : EPOCH - 1 : training on 50583 raw words (33305 effective words) took 0.4s, 74805 effective words/s\n",
      "2019-01-14 11:49:30,372 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:30,383 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:30,387 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:30,479 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:30,479 : INFO : EPOCH - 2 : training on 50583 raw words (33111 effective words) took 0.5s, 73033 effective words/s\n",
      "2019-01-14 11:49:30,846 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:30,850 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:30,851 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:30,939 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:30,940 : INFO : EPOCH - 3 : training on 50583 raw words (33099 effective words) took 0.5s, 72753 effective words/s\n",
      "2019-01-14 11:49:31,271 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:31,285 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:31,308 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:31,376 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:31,377 : INFO : EPOCH - 4 : training on 50583 raw words (33095 effective words) took 0.4s, 76845 effective words/s\n",
      "2019-01-14 11:49:31,718 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:31,724 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:31,729 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:31,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:31,823 : INFO : EPOCH - 5 : training on 50583 raw words (33109 effective words) took 0.4s, 75139 effective words/s\n",
      "2019-01-14 11:49:31,824 : INFO : training on a 252915 raw words (165719 effective words) took 2.3s, 73487 effective words/s\n",
      "2019-01-14 11:49:31,824 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-01-14 11:49:31,826 : INFO : training model with 4 workers on 305 vocabulary and 300 features, using sg=1 hs=1 sample=0.001 negative=10 window=8\n",
      "2019-01-14 11:49:32,158 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:32,168 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:32,172 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:32,266 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:32,267 : INFO : EPOCH - 1 : training on 50583 raw words (33124 effective words) took 0.4s, 76077 effective words/s\n",
      "2019-01-14 11:49:32,596 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:32,605 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:32,620 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:32,707 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:32,707 : INFO : EPOCH - 2 : training on 50583 raw words (33178 effective words) took 0.4s, 76384 effective words/s\n",
      "2019-01-14 11:49:33,059 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:33,060 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:33,066 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:33,154 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:33,155 : INFO : EPOCH - 3 : training on 50583 raw words (33060 effective words) took 0.4s, 74900 effective words/s\n",
      "2019-01-14 11:49:33,490 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:33,507 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:33,510 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:33,597 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:33,597 : INFO : EPOCH - 4 : training on 50583 raw words (33040 effective words) took 0.4s, 75705 effective words/s\n",
      "2019-01-14 11:49:33,918 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:33,926 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:33,930 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:34,021 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:34,022 : INFO : EPOCH - 5 : training on 50583 raw words (33190 effective words) took 0.4s, 79141 effective words/s\n",
      "2019-01-14 11:49:34,023 : INFO : training on a 252915 raw words (165592 effective words) took 2.2s, 75392 effective words/s\n",
      "2019-01-14 11:49:34,024 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-01-14 11:49:34,024 : INFO : training model with 4 workers on 305 vocabulary and 300 features, using sg=1 hs=1 sample=0.001 negative=10 window=8\n",
      "2019-01-14 11:49:34,356 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:34,359 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:34,374 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:34,458 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:34,459 : INFO : EPOCH - 1 : training on 50583 raw words (33115 effective words) took 0.4s, 77517 effective words/s\n",
      "2019-01-14 11:49:34,790 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:34,795 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:34,798 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:34,892 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:34,893 : INFO : EPOCH - 2 : training on 50583 raw words (33050 effective words) took 0.4s, 77435 effective words/s\n",
      "2019-01-14 11:49:35,203 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:35,210 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:35,218 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:35,310 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:35,310 : INFO : EPOCH - 3 : training on 50583 raw words (33104 effective words) took 0.4s, 80318 effective words/s\n",
      "2019-01-14 11:49:35,642 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:35,649 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:35,655 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 11:49:35,748 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:35,748 : INFO : EPOCH - 4 : training on 50583 raw words (33070 effective words) took 0.4s, 76480 effective words/s\n",
      "2019-01-14 11:49:36,087 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:36,091 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:36,094 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:36,187 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:36,187 : INFO : EPOCH - 5 : training on 50583 raw words (33112 effective words) took 0.4s, 76423 effective words/s\n",
      "2019-01-14 11:49:36,188 : INFO : training on a 252915 raw words (165451 effective words) took 2.2s, 76472 effective words/s\n",
      "2019-01-14 11:49:36,190 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-01-14 11:49:36,190 : INFO : training model with 4 workers on 305 vocabulary and 300 features, using sg=1 hs=1 sample=0.001 negative=10 window=8\n",
      "2019-01-14 11:49:36,519 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:36,526 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:36,529 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:36,622 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:36,623 : INFO : EPOCH - 1 : training on 50583 raw words (33089 effective words) took 0.4s, 77764 effective words/s\n",
      "2019-01-14 11:49:36,952 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:36,957 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:36,962 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:37,055 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:37,056 : INFO : EPOCH - 2 : training on 50583 raw words (33164 effective words) took 0.4s, 77481 effective words/s\n",
      "2019-01-14 11:49:37,379 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:37,388 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:37,396 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:37,485 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:37,485 : INFO : EPOCH - 3 : training on 50583 raw words (32928 effective words) took 0.4s, 78024 effective words/s\n",
      "2019-01-14 11:49:37,823 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:37,829 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:37,835 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:37,929 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:37,930 : INFO : EPOCH - 4 : training on 50583 raw words (33113 effective words) took 0.4s, 75609 effective words/s\n",
      "2019-01-14 11:49:38,254 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:38,259 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:38,262 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:38,358 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:38,359 : INFO : EPOCH - 5 : training on 50583 raw words (33038 effective words) took 0.4s, 78190 effective words/s\n",
      "2019-01-14 11:49:38,359 : INFO : training on a 252915 raw words (165332 effective words) took 2.2s, 76264 effective words/s\n",
      "2019-01-14 11:49:38,360 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-01-14 11:49:38,361 : INFO : training model with 4 workers on 305 vocabulary and 300 features, using sg=1 hs=1 sample=0.001 negative=10 window=8\n",
      "2019-01-14 11:49:38,686 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:38,692 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:38,701 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:38,793 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:38,794 : INFO : EPOCH - 1 : training on 50583 raw words (33120 effective words) took 0.4s, 77680 effective words/s\n",
      "2019-01-14 11:49:39,124 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:39,130 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:39,140 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:39,227 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:39,228 : INFO : EPOCH - 2 : training on 50583 raw words (33103 effective words) took 0.4s, 77350 effective words/s\n",
      "2019-01-14 11:49:39,564 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:39,566 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:39,573 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:39,667 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:39,667 : INFO : EPOCH - 3 : training on 50583 raw words (33177 effective words) took 0.4s, 76520 effective words/s\n",
      "2019-01-14 11:49:39,985 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:39,994 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:40,000 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:40,095 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:40,096 : INFO : EPOCH - 4 : training on 50583 raw words (33018 effective words) took 0.4s, 78137 effective words/s\n",
      "2019-01-14 11:49:40,428 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:40,434 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:40,437 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:40,529 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:40,530 : INFO : EPOCH - 5 : training on 50583 raw words (33005 effective words) took 0.4s, 77128 effective words/s\n",
      "2019-01-14 11:49:40,530 : INFO : training on a 252915 raw words (165423 effective words) took 2.2s, 76260 effective words/s\n",
      "2019-01-14 11:49:40,531 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-01-14 11:49:40,532 : INFO : training model with 4 workers on 305 vocabulary and 300 features, using sg=1 hs=1 sample=0.001 negative=10 window=8\n",
      "2019-01-14 11:49:40,866 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:40,870 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:40,877 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:40,967 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:40,967 : INFO : EPOCH - 1 : training on 50583 raw words (33267 effective words) took 0.4s, 77737 effective words/s\n",
      "2019-01-14 11:49:41,308 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:41,310 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:41,315 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:41,408 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:41,409 : INFO : EPOCH - 2 : training on 50583 raw words (33117 effective words) took 0.4s, 76003 effective words/s\n",
      "2019-01-14 11:49:41,718 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:41,734 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:41,735 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:41,824 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 11:49:41,825 : INFO : EPOCH - 3 : training on 50583 raw words (32992 effective words) took 0.4s, 80600 effective words/s\n",
      "2019-01-14 11:49:42,135 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:42,142 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:42,148 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:42,242 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:42,243 : INFO : EPOCH - 4 : training on 50583 raw words (33064 effective words) took 0.4s, 80192 effective words/s\n",
      "2019-01-14 11:49:42,561 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:42,574 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:42,579 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:42,676 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:42,676 : INFO : EPOCH - 5 : training on 50583 raw words (33020 effective words) took 0.4s, 77336 effective words/s\n",
      "2019-01-14 11:49:42,677 : INFO : training on a 252915 raw words (165460 effective words) took 2.1s, 77137 effective words/s\n",
      "2019-01-14 11:49:42,678 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-01-14 11:49:42,679 : INFO : training model with 4 workers on 305 vocabulary and 300 features, using sg=1 hs=1 sample=0.001 negative=10 window=8\n",
      "2019-01-14 11:49:42,999 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:43,006 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:43,012 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:43,102 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:43,102 : INFO : EPOCH - 1 : training on 50583 raw words (33246 effective words) took 0.4s, 79715 effective words/s\n",
      "2019-01-14 11:49:43,415 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:43,424 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:43,429 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:43,521 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:43,522 : INFO : EPOCH - 2 : training on 50583 raw words (33099 effective words) took 0.4s, 80155 effective words/s\n",
      "2019-01-14 11:49:43,844 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:43,851 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:43,854 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:43,944 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:43,945 : INFO : EPOCH - 3 : training on 50583 raw words (33084 effective words) took 0.4s, 79237 effective words/s\n",
      "2019-01-14 11:49:44,262 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:44,266 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:44,271 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:44,368 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:44,369 : INFO : EPOCH - 4 : training on 50583 raw words (33145 effective words) took 0.4s, 79504 effective words/s\n",
      "2019-01-14 11:49:44,696 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-14 11:49:44,703 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-14 11:49:44,706 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-14 11:49:44,799 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-14 11:49:44,800 : INFO : EPOCH - 5 : training on 50583 raw words (33042 effective words) took 0.4s, 77663 effective words/s\n",
      "2019-01-14 11:49:44,800 : INFO : training on a 252915 raw words (165616 effective words) took 2.1s, 78095 effective words/s\n",
      "2019-01-14 11:49:44,801 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During Time: 21.824445724487305\n"
     ]
    }
   ],
   "source": [
    "# 벡터 문서 학습\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(10):\n",
    "    doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n",
    "    doc_vectorizer.alpha -= 0.002 # decrease the learning rate\n",
    "    doc_vectorizer.min_alpha = doc_vectorizer.alpha # fix the learning rate, no decay\n",
    "end = time.time()\n",
    "print(\"During Time: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 11:49:56,096 : INFO : saving Doc2Vec object under Doc2vec(dbow+w,d300,n10,hs,w8,mc20,s0.001,t24).model, separately None\n",
      "2019-01-14 11:49:56,140 : INFO : saved Doc2vec(dbow+w,d300,n10,hs,w8,mc20,s0.001,t24).model\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Doc2vec(dbow+w,d300,n10,hs,w8,mc20,s0.001,t24).model'\n",
    "doc_vectorizer.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 11:50:01,261 : INFO : loading Doc2Vec object from Doc2vec(dbow+w,d300,n10,hs,w8,mc20,s0.001,t24).model\n",
      "2019-01-14 11:50:01,280 : INFO : loading vocabulary recursively from Doc2vec(dbow+w,d300,n10,hs,w8,mc20,s0.001,t24).model.vocabulary.* with mmap=None\n",
      "2019-01-14 11:50:01,281 : INFO : loading trainables recursively from Doc2vec(dbow+w,d300,n10,hs,w8,mc20,s0.001,t24).model.trainables.* with mmap=None\n",
      "2019-01-14 11:50:01,282 : INFO : loading wv recursively from Doc2vec(dbow+w,d300,n10,hs,w8,mc20,s0.001,t24).model.wv.* with mmap=None\n",
      "2019-01-14 11:50:01,284 : INFO : loading docvecs recursively from Doc2vec(dbow+w,d300,n10,hs,w8,mc20,s0.001,t24).model.docvecs.* with mmap=None\n",
      "2019-01-14 11:50:01,285 : INFO : loaded Doc2vec(dbow+w,d300,n10,hs,w8,mc20,s0.001,t24).model\n"
     ]
    }
   ],
   "source": [
    "doc_vectorizer = Doc2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('길', 0.24706873297691345), ('너무', 0.24242840707302094), ('다', 0.2405060976743698), ('지만', 0.23725199699401855), ('만', 0.2370227575302124), ('부분', 0.23059773445129395), ('기', 0.22941675782203674), ('좀', 0.2211339771747589), ('끝', 0.2189684361219406), ('ㄴ가', 0.2184128314256668)]\n"
     ]
    }
   ],
   "source": [
    "print(doc_vectorizer.wv.most_similar('지루'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_train_docs]\n",
    "y_train = [doc.tags for doc in tagged_train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3465 3465\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_np = np.asarray([0 if c == 'NEG' else 1 if c == 'NEU' else 2 for c in y_train], dtype=int)\n",
    "y_test_np = np.asarray([0 if c == 'NEG' else 1 if c == 'NEU' else 2 for c in y_test], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = np.asarray(X_train)\n",
    "X_test_np = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 300\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_np[0]), len(X_test_np[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3465, 300)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_np = np.eye(3)[y_train_np.reshape(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_np = np.eye(3)[y_train_np.reshape(-1)]\n",
    "y_test_np = np.eye(3)[y_test_np.reshape(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.338864s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print('Time: {:f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 정확도: 0.917\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(\"테스트 정확도: {:.3f}\".format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(50,),\n",
    "    max_iter=10,\n",
    "    alpha=1e-4,\n",
    "    solver='sgd',\n",
    "    verbose=10,\n",
    "    tol=1e-4,\n",
    "    random_state=1,\n",
    "    learning_rate_init=.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.44277698\n",
      "Iteration 2, loss = 0.34954099\n",
      "Iteration 3, loss = 0.34061573\n",
      "Iteration 4, loss = 0.33876536\n",
      "Iteration 5, loss = 0.33759062\n",
      "Iteration 6, loss = 0.33540616\n",
      "Iteration 7, loss = 0.33259323\n",
      "Iteration 8, loss = 0.32885835\n",
      "Iteration 9, loss = 0.32577958\n",
      "Iteration 10, loss = 0.32116787\n",
      "Time: 0.338864s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "mlp_clf.fit(X_train, y_train)\n",
    "print('Time: {:f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 정확도: 0.916\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlp_clf.predict(X_test)\n",
    "print(\"테스트 정확도: {:.3f}\".format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper Parameter\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "X = tf.placeholder(tf.float32, [None, 300])\n",
    "Y = tf.placeholder(tf.float32, [None, 3])\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Xavier_Initializer\n",
    "xavier_init = tf.contrib.layers.xavier_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layers and Output layer\n",
    "W1 = tf.get_variable(\"W1\", shape=[300, 256], initializer=xavier_init)\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1)+b1)\n",
    "dropout1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=xavier_init)\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(dropout1, W2)+b2)\n",
    "dropout2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 256], initializer=xavier_init)\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "L3 = tf.nn.relu(tf.matmul(dropout2, W3)+b3)\n",
    "dropout3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[256, 256], initializer=xavier_init)\n",
    "b4 = tf.Variable(tf.random_normal([256]))\n",
    "L4 = tf.nn.relu(tf.matmul(dropout3, W4)+b4)\n",
    "dropout4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[256, 3], initializer=xavier_init)\n",
    "b5 = tf.Variable(tf.random_normal([3]))\n",
    "hypothesis = tf.matmul(dropout4, W5)+b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-14 13:25:56,918 : WARNING : From <ipython-input-185-abdb2c63a065>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.405933570\n",
      "Epoch: 0002 cost = 0.575217735\n",
      "Epoch: 0003 cost = 0.370799499\n",
      "Epoch: 0004 cost = 0.369315606\n",
      "Epoch: 0005 cost = 0.335522826\n",
      "Epoch: 0006 cost = 0.319224849\n",
      "Epoch: 0007 cost = 0.313361711\n",
      "Epoch: 0008 cost = 0.323535832\n",
      "Epoch: 0009 cost = 0.370518668\n",
      "Epoch: 0010 cost = 0.311855858\n",
      "Epoch: 0011 cost = 0.311811406\n",
      "Epoch: 0012 cost = 0.298665226\n",
      "Epoch: 0013 cost = 0.287021327\n",
      "Epoch: 0014 cost = 0.288835754\n",
      "Epoch: 0015 cost = 0.285211912\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(len(X_train_np) / batch_size)\n",
    "\n",
    "    for i in range(0, len(X_train_np), batch_size):\n",
    "        batch_xs = X_train_np[i:i+batch_size]\n",
    "        batch_ys = y_train_np[i:i+batch_size]\n",
    "\n",
    "        feed_dict = {X:batch_xs, Y: batch_ys, keep_prob:0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '{:04d}'.format(epoch +1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Training Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 정확도:  0.9423077\n"
     ]
    }
   ],
   "source": [
    "# Test Model and check Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('테스트 정확도: ', sess.run(accuracy, feed_dict={X: X_test_np ,Y:y_test_np, keep_prob:1 }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected, batch_norm, dropout\n",
    "from tensorflow.contrib.framework import arg_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "keep_prob = 0.7\n",
    "\n",
    "# Input Layer\n",
    "X = tf.placeholder(tf.float32, [None, 300])\n",
    "Y = tf.placeholder(tf.float32, [None, 3])\n",
    "train_mode = tf.placeholder(tf.bool, name='train_mode')\n",
    "\n",
    "# Layer output size\n",
    "hidden_output_size = 300\n",
    "final_ouput_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ouput_size = 3\n",
    "bn_params = {\n",
    "    'is_training': train_mode,\n",
    "    'decay':0.9,\n",
    "    'updates_collections':None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "with arg_scope(\n",
    "    [fully_connected],\n",
    "    activation_fn=tf.nn.relu,\n",
    "    weights_initializer = xavier_init,\n",
    "    biases_initializer=None,\n",
    "    normalizer_fn=batch_norm,\n",
    "    normalizer_params=bn_params):\n",
    "\n",
    "    h1 = fully_connected(X, hidden_output_size, scope='h1')\n",
    "    dropout1 = dropout(h1, keep_prob, is_training=train_mode)\n",
    "\n",
    "    h2 = fully_connected(dropout1, hidden_output_size, scope='h2')\n",
    "    dropout2 = dropout(h2, keep_prob, is_training=train_mode)\n",
    "\n",
    "    h3 = fully_connected(dropout2, hidden_output_size, scope='h3')\n",
    "    dropout3 = dropout(h3, keep_prob, is_training=train_mode)\n",
    "\n",
    "    h4 = fully_connected(dropout3, hidden_output_size, scope='h4')\n",
    "    dropout4 = dropout(h4, keep_prob, is_training=train_mode)\n",
    "\n",
    "    hypothesis = fully_connected(dropout4, final_ouput_size, activation_fn=None, scope='hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Cost/Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 cost=0.254040423\n",
      "Epoch:    2 cost=0.253077908\n",
      "Epoch:    3 cost=0.249403213\n",
      "Epoch:    4 cost=0.249401812\n",
      "Epoch:    5 cost=0.244848818\n",
      "Epoch:    6 cost=0.238808088\n",
      "Epoch:    7 cost=0.238551485\n",
      "Epoch:    8 cost=0.237359939\n",
      "Epoch:    9 cost=0.239333050\n",
      "Epoch:   10 cost=0.235812906\n",
      "Epoch:   11 cost=0.232479444\n",
      "Epoch:   12 cost=0.231756500\n",
      "Epoch:   13 cost=0.234103864\n",
      "Epoch:   14 cost=0.232549542\n",
      "Epoch:   15 cost=0.226675212\n",
      "Learning Finished\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(len(X_train_np)/batch_size)\n",
    "\n",
    "    for i in range(0, len(X_train_np), batch_size):\n",
    "        batch_xs = X_train_np[i:i+batch_size]\n",
    "        batch_ys = y_train_np[i:i+batch_size]\n",
    "\n",
    "        feed_dict_train = {X: batch_xs, Y: batch_ys, train_mode:True }\n",
    "        feed_dict_cost = {X: batch_xs, Y: batch_ys, train_mode:False}\n",
    "\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict_train)\n",
    "        avg_cost += c / total_batch\n",
    "    print(\"Epoch: {:4d} cost={:.9f}\".format(epoch+1, avg_cost))\n",
    "\n",
    "print(\"Learning Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95384616\n"
     ]
    }
   ],
   "source": [
    "# Test Model and Check Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict= {X:X_test_np, Y:y_test_np, train_mode:False}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95384616"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(accuracy, feed_dict= {X:X_test_np, Y:y_test_np, train_mode:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = saver.save(sess, './model.pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
